---
title: Hadoop.md
date: 2023-08-25 08:32:11
tags:Bigdata
---

Manage tool: Ambari+Bigtop
HDFS/YARN/MapReduce2/Tez/Hive/HBase/ZooKeeper/Spark/Zeppelin/Flink

Flink-cdc/datax/dolphinscheduler

<!-- more -->



## Ambari

The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.

***Notice: Bigtop repository has included all of ambari packages, you don't need to build. just need to build the latest version that bigtop not included.***

For installation, please follow this instructions: [Installation Guide for Ambari 2.8.0 - Apache Ambari - Apache Software Foundation](https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.8.0)

### Build  package from source

Prerequisite:

```bash
#Jdk
nvm install v12.22.1

cat /etc/profile.d/java.sh 
#!/bin/bash

export JAVA_HOME=/Developer/jdk1.8.0_371
export M2_HOME=/Developer/apache-maven-3.6.3
export _JAVA_OPTIONS="-Xms4g -Xmx4g -Djava.awt.headless=true"
export PATH=/root/.nvm/versions/node/v12.22.1/bin:$JAVA_HOME/bin:$M2_HOME/bin:$PATH

. /etc/profile

#OS environment:
#swap>=6G:
dd if=/dev/zero of=/myswap.swp bs=1k count=4194304 #The vm has been included 2g memory. 
mkswap /myswap.swp
swapon /myswap.swp
free -m
chmod +x /etc/rc.local
chmod +x /etc/rc.d/rc.local

echo "swapon /myswap.swp" >> /etc/rc.local

groupadd hadoop
useradd -m -g hadoop hadoop
passwd hadoop
chmod +w /etc/sudoers
echo "hadoop ALL=(ALL)NOPASSWD: ALL" >> /etc/sudoers
chmod -w /etc/sudoers
```

Build package from source

```
#https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.8.0
Centos 7.9:
yum install -y git python-devel rpm-build gcc-c++

wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg#md5=fe1f997bc722265116870bc7919059ea
sh setuptools-0.6c11-py2.7.egg

wget https://dlcdn.apache.org/ambari/ambari-2.8.0/apache-ambari-2.8.0-src.tar.gz (use the suggested mirror from above)
tar xfvz apache-ambari-2.8.0-src.tar.gz
cd apache-ambari-2.8.0-src
mvn clean install rpm:rpm -DskipTests -Drat.skip=true
```

Build your yum repository:

See: [bigtop Section](#Bigtop)



### Installing Ambari

```
Omitted...
```



## Bigtop

Bigtop is an Apache Foundation project for Infrastructure Engineers and Data Scientists looking for comprehensive packaging, testing, and configuration of the leading open source big data components.** Bigtop supports a wide range of components/projects, including, but not limited to, Hadoop, HBase and Spark.



There are 2 ways to install bigtop:

### build package from source

***Not recommend, it's very complicate. especially in China mainland.

Prerequisite:

```bash
#Jdk
nvm install v12.22.1

cat /etc/profile.d/java.sh 
#!/bin/bash

export JAVA_HOME=/Developer/jdk1.8.0_371
export M2_HOME=/Developer/apache-maven-3.6.3
export _JAVA_OPTIONS="-Xms4g -Xmx4g -Djava.awt.headless=true"
export PATH=/root/.nvm/versions/node/v12.22.1/bin:$JAVA_HOME/bin:$M2_HOME/bin:$PATH

. /etc/profile
```

Building:

***Notice: Need a non-root to compile.***

```
sudo su - hadoop
wget https://dlcdn.apache.org/bigtop/bigtop-3.2.0/bigtop-3.2.0-project.tar.gz (use the suggested mirror from above)
tar xfvz bigtop-3.2.0-project.tar.gz
cd bigtop-3.2.0
#only for rpm packages
./gradlew bigtop-groovy-rpm bigtop-jsvc-rpm bigtop-select-rpm bigtop-utils-rpm \
flink-rpm hadoop-rpm hbase-rpm hive-rpm kafka-rpm solr-rpm spark-rpm \
tez-rpm zeppelin-rpm zookeeper-rpm -Dbuildwithdeps=true -PparentDir=/usr/bigtop -PpkgSuffix | tee -a log.txt
#it'll clean all of packages located inbuild/, be careful!
#./gradlew allclean 
```

Troubleshooting:

```
#lacking some of jars
wget https://www.zhangjc.com/images/20210817/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
mvn install:install-file -Dfile=./pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar -DgroupId=org.pentaho -DartifactId=pentaho-aggdesigner-algorithm -Dversion=5.1.5-jhyde -Dpackaging=jar

wget https://packages.confluent.io/maven/io/confluent/kafka-schema-registry-client/6.2.2/kafka-schema-registry-client-6.2.2.jar
mvn install:install-file -Dfile=./kafka-schema-registry-client-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-schema-registry-client -Dversion=6.2.2 -Dpackaging=jar
mvn install:install-file -Dfile=./kafka-clients-2.8.1.jar -DgroupId=org.apache.kafka -DartifactId=kafka-clients -Dversion=2.8.1 -Dpackaging=jar

wget https://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/6.2.2/kafka-avro-serializer-6.2.2.jar
mvn install:install-file -Dfile=./kafka-avro-serializer-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-avro-serializer -Dversion=6.2.2 -Dpackaging=jar


cd dl/
tar zxf flink-1.15.3.tar.gz
rm -fr flink-1.15.3/flink-formats/flink-avro-confluent-registry/src/test/
rm -fr flink-1.15.3/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test
rm -fr flink-1.15.3.tar.gz
tar -zcf flink-1.15.3.tar.gz flink-1.15.3
rm -fr flink-1.15.3
rm -fr /Developer/bigtop-3.2.0/build/flink/


tar zxf hadoop-3.3.4.tar.gz
vim hadoop-3.3.4-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xm
<nodejs.version>v14.0.0</nodejs.version>
rm -fr hadoop-3.3.4.tar.gz && tar -zcf hadoop-3.3.4.tar.gz hadoop-3.3.4-src && rm -fr hadoop-3.3.4-src
rm -fr /Developer/bigtop-3.2.0/build/hadoop/
```



### bigtop Repositories

It's a easy way to install, including ambari packages:

```bash
#Clone to local repository:
wget https://dlcdn.apache.org/bigtop/bigtop-3.2.1/repos/centos-7/bigtop.repo -O /etc/yum.repos.d/bigtop.repo
reposync --gpgcheck -1 --repoid=bigtop --download_path=/data/bigtop
cd /data/bigtop/bigtop
yum install createrepo
createrepo .

tree ./
.
├── bigtop
│   ├── alluxio
│   │   └── x86_64
│   │       └── alluxio-2.8.0-2.el7.x86_64.rpm
│   ├── ambari
│   │   ├── noarch
│   │   │   ├── ambari-agent-2.7.5.0-1.el7.noarch.rpm
│   │   │   └── ambari-server-2.7.5.0-1.el7.noarch.rpm
│   │   └── x86_64
│   │       ├── ambari-metrics-collector-2.7.5.0-0.x86_64.rpm
│   │       ├── ambari-metrics-grafana-2.7.5.0-0.x86_64.rpm
│   │       ├── ambari-metrics-hadoop-sink-2.7.5.0-0.x86_64.rpm
│   │       └── ambari-metrics-monitor-2.7.5.0-0.x86_64.rpm
│   ├── bigtop-ambari-mpack
│   │   └── noarch
│   │       └── bigtop-ambari-mpack-2.7.5.0-1.el7.noarch.rpm
│   ├── bigtop-groovy
│   │   └── noarch
│   │       └── bigtop-groovy-2.5.4-1.el7.noarch.rpm
│   ├── bigtop-jsvc
│   │   └── x86_64
│   │       ├── bigtop-jsvc-1.2.4-1.el7.x86_64.rpm
│   │       └── bigtop-jsvc-debuginfo-1.2.4-1.el7.x86_64.rpm
│   ├── bigtop-utils
│   │   └── noarch
│   │       └── bigtop-utils-3.2.1-1.el7.noarch.rpm
│   ├── flink
│   │   └── noarch
│   │       ├── flink-1.15.3-1.el7.noarch.rpm
│   │       ├── flink-jobmanager-1.15.3-1.el7.noarch.rpm
│   │       └── flink-taskmanager-1.15.3-1.el7.noarch.rpm
│   ├── gpdb
│   │   └── x86_64
│   │       └── gpdb-5.28.5-1.el7.x86_64.rpm
│   ├── hadoop
│   │   └── x86_64
│   │       ├── hadoop-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-client-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-conf-pseudo-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-debuginfo-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-doc-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-datanode-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-dfsrouter-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-fuse-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-journalnode-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-namenode-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-secondarynamenode-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-hdfs-zkfc-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-httpfs-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-kms-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-libhdfs-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-libhdfs-devel-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-libhdfspp-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-libhdfspp-devel-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-mapreduce-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-mapreduce-historyserver-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-yarn-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-yarn-nodemanager-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-yarn-proxyserver-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-yarn-resourcemanager-3.3.5-1.el7.x86_64.rpm
│   │       ├── hadoop-yarn-router-3.3.5-1.el7.x86_64.rpm
│   │       └── hadoop-yarn-timelineserver-3.3.5-1.el7.x86_64.rpm
│   ├── hbase
│   │   ├── noarch
│   │   │   └── hbase-doc-2.4.13-2.el7.noarch.rpm
│   │   └── x86_64
│   │       ├── hbase-2.4.13-2.el7.x86_64.rpm
│   │       ├── hbase-master-2.4.13-2.el7.x86_64.rpm
│   │       ├── hbase-regionserver-2.4.13-2.el7.x86_64.rpm
│   │       ├── hbase-rest-2.4.13-2.el7.x86_64.rpm
│   │       ├── hbase-thrift2-2.4.13-2.el7.x86_64.rpm
│   │       └── hbase-thrift-2.4.13-2.el7.x86_64.rpm
│   ├── hive
│   │   └── noarch
│   │       ├── hive-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-hbase-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-hcatalog-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-hcatalog-server-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-jdbc-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-metastore-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-server2-3.1.3-1.el7.noarch.rpm
│   │       ├── hive-webhcat-3.1.3-1.el7.noarch.rpm
│   │       └── hive-webhcat-server-3.1.3-1.el7.noarch.rpm
│   ├── kafka
│   │   └── noarch
│   │       ├── kafka-2.8.1-2.el7.noarch.rpm
│   │       └── kafka-server-2.8.1-2.el7.noarch.rpm
│   ├── livy
│   │   └── noarch
│   │       └── livy-0.7.1-1.el7.noarch.rpm
│   ├── oozie
│   │   └── noarch
│   │       ├── oozie-5.2.1-2.el7.noarch.rpm
│   │       └── oozie-client-5.2.1-2.el7.noarch.rpm
│   ├── phoenix
│   │   └── noarch
│   │       └── phoenix-5.1.2-1.el7.noarch.rpm
│   ├── solr
│   │   └── noarch
│   │       ├── solr-8.11.2-1.el7.noarch.rpm
│   │       ├── solr-doc-8.11.2-1.el7.noarch.rpm
│   │       └── solr-server-8.11.2-1.el7.noarch.rpm
│   ├── spark
│   │   └── noarch
│   │       ├── spark-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-core-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-datanucleus-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-external-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-history-server-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-master-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-python-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-sparkr-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-thriftserver-3.2.3-1.el7.noarch.rpm
│   │       ├── spark-worker-3.2.3-1.el7.noarch.rpm
│   │       └── spark-yarn-shuffle-3.2.3-1.el7.noarch.rpm
│   ├── tez
│   │   └── noarch
│   │       └── tez-0.10.1-1.el7.noarch.rpm
│   ├── ycsb
│   │   └── noarch
│   │       └── ycsb-0.17.0-2.el7.noarch.rpm
│   ├── zeppelin
│   │   └── x86_64
│   │       └── zeppelin-0.10.1-1.el7.x86_64.rpm
│   └── zookeeper
│       └── x86_64
│           ├── zookeeper-3.5.9-2.el7.x86_64.rpm
│           ├── zookeeper-debuginfo-3.5.9-2.el7.x86_64.rpm
│           ├── zookeeper-native-3.5.9-2.el7.x86_64.rpm
│           ├── zookeeper-rest-3.5.9-2.el7.x86_64.rpm
│           └── zookeeper-server-3.5.9-2.el7.x86_64.rpm
```



## Hadoop

The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.

The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.



### File System Shell

[File System Shell Manual](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)



HDFS

MapReduce2

YARN

## Hive

The Apache Hive ™ is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale and facilitates reading, writing, and managing petabytes of data residing in distributed storage using SQL.

[GettingStarted](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)

### internal table

If table has beed deleted, all data will be delete accordingly, including meta data and file data.

```
#https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML
#LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
#filepath can be:
#a relative path, such as project/data1
#an absolute path, such as /user/hive/project/data1
#a full URI with scheme and (optionally) an authority, such as hdfs://namenode:9000/user/hive/project/data1
The keyword 'OVERWRITE' signifies that existing data in the table is deleted. If the 'OVERWRITE' keyword is omitted, data files are appended to existing data sets.

#default as internal table: 
CREATE TABLE pokes (foo INT, bar STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

sudo -u hive hadoop fs -put -f /tmp/kv1.txt /user/hive/demo/
LOAD DATA INPATH './demo/kv1.txt' OVERWRITE INTO TABLE pokes;
#When it's done, the file located in hdfs will be deleted.
select * from pokes;

```

### external table

If table has beed deleted, just meta data will be deleted. once you create table again, the data will be restored, no need load again.

```
#sudo -u hdfs hadoop fs -chown -R hive:hive /works/test/
#sudo -u hive hadoop fs -cp  /user/hive/demo/kv1.txt /works/test/
sudo -u hive hadoop fs -put -f /tmp/kv1.txt /works/demo/
create external table mytest ( id int, myfields string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE location '/works/test/';
LOAD DATA INPATH '/works/demo/kv1.txt' OVERWRITE INTO TABLE mytest;
describe formatted mytest;
```

### Partition

```
CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;
#sudo -u hive hadoop fs -put -f /tmp/kv1.txt /user/hive/demo/
LOAD DATA INPATH './demo/kv1.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');
select * from invites;
SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';
```

### Insert Directory

```
#selects all rows from partition ds=2008-08-15 of the invites table into an HDFS directory. The result data is in files (depending on the number of mappers) in that directory.
NOTE: partition columns if any are selected by the use of *. They can also be specified in the projection clauses.

INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';
#local dirctory located on the same node of hiveserver2.
INSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a;
```

### Insert Table

```
CREATE TABLE events (foo INT, bar STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;
INSERT OVERWRITE TABLE events SELECT a.* FROM pokes a;
FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo > 0 GROUP BY a.bar;
INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo > 0 GROUP BY a.bar;
```



### Date Type

[Hive 数据类型 | Hive 教程 (hadoopdoc.com)](https://hadoopdoc.com/hive/hive-data-type)

#### STRUCT

```
CREATE TABLE IF NOT EXISTS person_1 (id int,info struct<name:string,country:string>)  
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
COLLECTION ITEMS TERMINATED BY ':' 
STORED AS TEXTFILE;

//创建一个文本文件test_struct.txt
1,'dd':'jp'
2,'ee':'cn'
3,'gg':'jp'
4,'ff':'cn'
5,'tt':'jp'

sudo -u hive hadoop fs -put /works/test/test_struct.txt /user/hive/demo/
LOAD DATA INPATH './demo/test_struct.txt' OVERWRITE INTO TABLE person_1;

select * from person_1;
+--------------+-----------------------------------+
| person_1.id  |           person_1.info           |
+--------------+-----------------------------------+
| 1            | {"name":"'dd'","country":"'jp'"}  |
| 2            | {"name":"'ee'","country":"'cn'"}  |
| 3            | {"name":"'gg'","country":"'jp'"}  |
| 4            | {"name":"'ff'","country":"'cn'"}  |
| 5            | {"name":"'tt'","country":"'jp'"}  |
+--------------+------------------------

select id,info.name,info.country from person_1 where info.name='\'dd\'';
+-----+-------+----------+
| id  | name  | country  |
+-----+-------+----------+
| 1   | 'dd'  | 'jp'     |
+-----+-------+----------+
1 row selected (0.316 seconds)
```

#### ARRAY

```
CREATE TABLE IF NOT EXISTS array_1 (id int,name array<STRING>)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
COLLECTION ITEMS TERMINATED BY ':' 
STORED AS TEXTFILE;
//导入数据
sudo -u hive hadoop fs -put /works/test/test_struct.txt /user/hive/demo/test_array.txt
LOAD DATA INPATH './demo/test_array.txt' OVERWRITE INTO TABLE array_1;
//查询数据
hive> select * from array_1;
OK
1   ["dd","jp"]
2   ["ee","cn"]
3   ["gg","jp"]
4   ["ff","cn"]
5   ["tt","jp"]
Time taken: 0.041 seconds, Fetched: 5 row(s)
hive> select id,name[0],name[1] from array_1 where name[1]='\'cn\'';
+-----+-------+-------+
| id  |  _c1  |  _c2  |
+-----+-------+-------+
| 2   | 'ee'  | 'cn'  |
| 4   | 'ff'  | 'cn'  |
+-----+-------+-------+
2 rows selected (0.317 seconds)
```

#### MAP

```
CREATE TABLE IF NOT EXISTS map_1 (id int,name map<STRING,STRING>)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' 
COLLECTION ITEMS TERMINATED BY ',' 
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;

cat test_map.txt    
1|'name':'jp','country':'cn'
2|'name':'jp','country':'cn'

sudo -u hive hadoop fs -put /works/test/test_map.txt /user/hive/demo/test_map.txt
//加载数据
LOAD DATA INPATH './demo/test_map.txt' OVERWRITE INTO TABLE map_1;

//查询数据
hive> select * from map_1;
+-----------+---------------------------------------+
| map_1.id  |              map_1.name               |
+-----------+---------------------------------------+
| 1         | {"'name'":"'jp'","'country'":"'cn'"}  |
| 2         | {"'name'":"'jp'","'country'":"'cn'"}  |
+-----------+-----------------------------------
hive> select id,name["'name'"],name["'country'"] from map_1;
+-----+-------+-------+
| id  |  _c1  |  _c2  |
+-----+-------+-------+
| 1   | 'jp'  | 'cn'  |
| 2   | 'jp'  | 'cn'  |
+-----+-------+-------+
hive> select * from map_1 where name["'country'"]='\'cn\'';
+-----------+---------------------------------------+
| map_1.id  |              map_1.name               |
+-----------+---------------------------------------+
| 1         | {"'name'":"'jp'","'country'":"'cn'"}  |
| 2         | {"'name'":"'jp'","'country'":"'cn'"}  |
+-----------+---------------------------------------+
2 rows selected (0.287 seconds)

hive> insert into map_1(id,name)values(1, str_to_map("name:jp1,country:cn1")),(2, str_to_map("name:jp2,country:cn2"));
No rows affected (11.664 seconds)
hive> select * from map_1;
+-----------+---------------------------------------+
| map_1.id  |              map_1.name               |
+-----------+---------------------------------------+
| 1         | {"name":"jp1","country":"cn1"}        |
| 2         | {"name":"jp2","country":"cn2"}        |
+-----------+---------------------------------------+
4 rows selected (0.482 seconds)
```

### UINON

```

```

