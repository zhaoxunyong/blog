CD:

1. ArgoCD <--- ok
2. FluxCD <--- Not Recommend
3. JenkinsX <--- Not Recommend
4. aliyun
5. GitlabCI <--- ok
6. tekton <--- based on k8s, more completed than others.


Prometheus
kustomize <--- ok

rancher <--- ok
k3s/AutoK3s <--- ok
---------------------------------------------------------------------------------
K3s:(Recommend)
#https://docs.k3s.io/quick-start
#https://github.com/k3s-io/k3s/issues/1160
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --disable traefik" sh
cp -a /etc/rancher/k3s/k3s.yaml ~/.kube/config
sed -i 's;127.0.0.1;192.168.101.82;g' ~/.kube/config
kubectl get all -A -o wide

#https://blog.thenets.org/how-to-create-a-k3s-cluster-with-nginx-ingress-controller/
#https://blog.csdn.net/weixin_45444133/article/details/116952250
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml

#Unstalling k3s
/usr/local/bin/k3s-uninstall.sh
---------------------------------------------------------------------------------
#AutoK3s:(Not Recommend)
##https://docs.rancher.cn/docs/k3s/autok3s/_index/
##https://jasonkayzk.github.io/2022/10/22/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2autok3s/

#For docker:(Recommend)
##docker run -itd --restart=unless-stopped --net host -v /var/run/docker.sock:/var/run/docker.sock cnrancher/autok3s:v0.6.0

##Installing on hosted machine:(optional)
#curl -sS https://rancher-mirror.rancher.cn/autok3s/install.sh  | INSTALL_AUTOK3S_MIRROR=cn sh
##Starting
#autok3s serve --bind-address 192.168.101.82 --bind-port 8080
##Uninstalling:
#/usr/local/bin/autok3s-uninstall.sh

##Install instance:
#Put "--disable traefik" param into "Master Extra Args"
##execute once:
#k get ns --insecure-skip-tls-verify
#k get ns`
---------------------------------------------------------------------------------
# Create a test Namespace, if not exist
kubectl create namespace test

# Apply the example file
#https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/
kubectl -n test apply -f my-example.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-nginx-app
  namespace: test
spec:
  selector:
    matchLabels:
      name: test-nginx-backend
  template:
    metadata:
      labels:
        name: test-nginx-backend
    spec:
      containers:
        - name: backend
          image: docker.io/nginx:alpine
          imagePullPolicy: Always
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: test-nginx-service
  namespace: test
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    name: test-nginx-backend

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-nginx-ingress
  namespace: test
spec:
  rules:
  - host: test.w1.thenets.org
    http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: test-nginx-service
              port: 
                number: 80
  ingressClassName: nginx


#argocd
#https://argo-cd.readthedocs.io/en/stable/getting_started/
#https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&mid=2247512193&idx=1&sn=da41bb4072870e34bdf338c22bcbc8cc&chksm=fbedf04ccc9a795a08f4b0deb5a8518aa901dc1e8678277d232fff0d05ba1613a3f8d8636ab9&scene=178&cur_album_id=2470838961377427457#rd

kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

#Service Type Load BalancerÂ¶
#Change the argocd-server service type to LoadBalancer:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
kubectl -n argocd get svc

#Change port to 8443 and 8080
kubectl -n argocd edit svc argocd-serve
  ports:
  - name: http
    nodePort: 31291
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: https
    nodePort: 31592
    port: 8443
    protocol: TCP
    targetPort: 8080


https://192.168.101.82:8443/

get password:
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo

login name is admin

git clone http://gitlab.zerofinance.net/dave.zhao/fleet_demo.git

#argo-rollouts
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

#demo
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml

#Watch
kubectl argo rollouts get rollout rollouts-demo -w

#Updating a Rollout
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:yellow

#Promoting a Rollout
kubectl argo rollouts promote rollouts-demo

#Updating a red Rollout
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:red

#Aborting a Rollout
kubectl argo rollouts abort rollouts-demo

#In order to make Rollout considered Healthy again and not Degraded, it is necessary to change the desired state back to the previous, stable versio
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:yellow
  

#ingress:
#https://argoproj.github.io/argo-rollouts/getting-started/nginx/
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml

#cat ingress.yaml has to be changed from >=1.19 cluster
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rollouts-demo-stable
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: rollouts-demo.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field
          service:
            name: rollouts-demo-stable
            port:


#Perform an update
kubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow


#dashboard
kubectl argo rollouts dashboard
http://192.168.101.82:3100/rollouts

#bluegreen
#https://github.com/argoproj/argo-rollouts/blob/master/examples/rollout-bluegreen.yaml

#cat rollout-bluegreen.yaml
# This example demonstrates a Rollout using the blue-green update strategy, which contains a manual
# gate before promoting the new stack.
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: rollout-bluegreen
spec:
  replicas: 2
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: rollout-bluegreen
  template:
    metadata:
      labels:
        app: rollout-bluegreen
    spec:
      containers:
      - name: rollouts-demo
        image: argoproj/rollouts-demo:blue
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
  strategy:
    blueGreen: 
      # activeService specifies the service to update with the new template hash at time of promotion.
      # This field is mandatory for the blueGreen update strategy.
      activeService: rollout-bluegreen-active
      # previewService specifies the service to update with the new template hash before promotion.
      # This allows the preview stack to be reachable without serving production traffic.
      # This field is optional.
      previewService: rollout-bluegreen-preview
      # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout
      # immediately before the promotion. If omitted, the default behavior is to promote the new
      # stack as soon as the ReplicaSet are completely ready/available.
      # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT`
      autoPromotionEnabled: false

---
kind: Service
apiVersion: v1
metadata:
  name: rollout-bluegreen-active
spec:
  selector:
    app: rollout-bluegreen
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

---
kind: Service
apiVersion: v1
metadata:
  name: rollout-bluegreen-preview
spec:
  selector:
    app: rollout-bluegreen
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rollouts-bluegreen-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: rollouts-bluegreen.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field
          service:
            name: rollout-bluegreen-active
            port:
              number: 80


kubectl argo rollouts set image rollout-bluegreen \
  rollouts-demo=argoproj/rollouts-demo:yellow

  kubectl argo rollouts get rollout rollout-bluegreen -w



#https://docs.gitlab.com/runner/install/docker.html
#https://docs.gitlab.com/runner/register/index.html#docker

#cat /etc/gitlab-runner/config.toml
concurrent = 1
check_interval = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "my-runner"
  url = "http://gitlab.zerofinance.net/"
  token = "111111"
  executor = "docker"
  [runners.custom_build_dir]
  [runners.cache]
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
  [runners.docker]
    tls_verify = false
    image = "docker:latest"
    privileged = true
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/var/run/docker.sock:/var/run/docker.sock","/cache","/works/config/runner:/runner"]
    shm_size = 0

docker run -d --name gitlab-runner --restart always \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /works/config/runner:/etc/gitlab-runner \
  gitlab/gitlab-runner:latest

#Register runner
docker run --rm -it -v /works/config/runner:/etc/gitlab-runner gitlab/gitlab-runner register

#https://blog.csdn.net/lenkty/article/details/124668164
#https://blog.csdn.net/boling_cavalry/article/details/106991691
#https://blog.csdn.net/sandaawa/article/details/112897733
#https://github.com/lonly197/docs/blob/master/src/operation/GitLab%20CI%20%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90.md
curl -X POST \
     -F token=a27c33cb07b863f0ecfc80f3650b50 \
     -F ref=1.0.x \
     -F variables[project]=alertmanager-webhook \
     http://gitlab.zerofinance.net/api/v4/projects/575/trigger/pipeline
     

Installing on Linux:
#https://docs.gitlab.com/runner/install/linux-manually.html
sudo curl -L --output /usr/local/bin/gitlab-runner "https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64"
sudo chmod +x /usr/local/bin/gitlab-runner
sudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash
sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner
sudo gitlab-runner start
#Remember git password,login as gitlab-runner:
git config --global credential.helper store
#Mocking cloning a certain repo, input username and password to store credential.

#https://docs.gitlab.com/runner/shells/index.html#shell-profile-loading
To troubleshoot this error, check /home/gitlab-runner/.bash_logout. For example, if the .bash_logout file has a script section like the following, comment it out and restart the pipeline:

if [ "$SHLVL" = 1 ]; then
    [ -x /usr/bin/clear_console ] && /usr/bin/clear_console -q
fi

#gitlab-runner register --name native-runner --url http://gitlab.zerofinance.net/ --registration-token 1111111111

choice shell as a executor.

sudo gitlab-runner register

--------------------------------------------------------------------------------------------------
#AMBARI Ubuntu(didn't work):
##https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Development
#apt-get install build-essential
#apt-get install rpm
#
#python2:
#wget https://www.python.org/ftp/python/2.7.17/Python-2.7.17.tg
#tar zxvf Python-2.7.17.tgz
#cd Python-2.7.17/
#./configure
#make
#make install
#ln -s /usr/local/bin/python /usr/bin/python
##python --version
##Python 2.7.17

##https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.7
#wget https://www-eu.apache.org/dist/ambari/ambari-2.7.7/apache-ambari-2.7.7-src.tar.gz
#tar xfvz apache-ambari-2.7.7-src.tar.gz
#cd apache-ambari-2.7.7-src
#mvn versions:set -DnewVersion=2.7.7.0.0
# 
#pushd ambari-metrics
#mvn versions:set -DnewVersion=2.7.7.0.0
#popd
-------------------------------------------

ä¿®æ¹settings.xmlï¼ #https://gist.github.com/reedv/2d6805e67b93fa7c743c4ff6c8e345dc
    <mirrors>
        <mirror>
            <id>nexus</id>
            <name>internalNexusRepository</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url> 
            <mirrorOf>central</mirrorOf>
        </mirror>
        <mirror>
                <id>nexus-hortonworks</id>
                <mirrorOf>*,!central</mirrorOf>
                <name>Nexus hortonworks</name>
                <url>https://repo.hortonworks.com/content/groups/public/</url>
        </mirror>
    </mirrors>

#ä¿®æ¹ï¼
##https://stackoverflow.com/questions/61654584/ambari-admin-view-2-7-5-0-0-build-failure
#ambari-admin/pom.xmlï¼
#<nodeVersion>v4.5.0</nodeVersion>æ¹ä¸ºï¼
#<nodeVersion>v11.10.0</nodeVersion>
#
##https://www.cnblogs.com/shine-rainbow/p/16149430.html
#
#æ æ³è®¿é®org.json.simple.JSONAware
#<dependency>
#      <groupId>com.googlecode.json-simple</groupId>
#      <artifactId>json-simple</artifactId>
#      <version>1.1</version>
#</dependency>
#
##ç±äºç¼è¯ç¯å¢æå³jaråä¸å®¹ææ¾å°(storm-core-0.10.0.2.3.0.0-2557.jarãzookeeper-3.4.5.1.3.0.0-107.jarãzookeeper-3.4.6.2.3.0.0-2557.jar)ã
##è´´åºambariç¼è¯çmaven repository,å°è¯¥repositoryæ¿æ¢æ.m2/repostiroyç®å½å³å¯ã
##scp -r repository.tar.gz root@192.168.101.82:/Developer/.m2/
#cd /Developer/.m2/
#tar zxvf repository.tar.gz
#
#wget https://repo.hortonworks.com/content/repositories/releases/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/storm-core-0.10.0.2.3.0.0-2557.jar -O /Developer/.m2/repository/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/storm-core-0.10.0.2.3.0.0-2557.jar
#ç¡®ä¿/Developer/.m2/repository/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/ä¸é¢åªæä¸ä¸ªjaræä»¶
#ç¡®ä¿/Developer/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5.1.3.0.0-107/ä¸é¢åªæä¸ä¸ªjaræä»¶
#
#cd /data/ambari/apache-ambari-2.7.7-src/
##mvn clean
#mvn -B clean install jdeb:jdeb -DnewVersion=2.7.7.0.0 -DbuildNumber=388e072381e71c7755673b7743531c03a4d61be8 -DskipTests -Drat.skip -Dpython.ver="python >= 2.6"


--------------------------------------------
#cd /data/vagrant/boxes/docker/ubuntu22.04/
#vagrant up
#
##login 192.168.101.83
#cd /vagrant/
#apt install python2 postgresql
#dpkg -i ambari-server_2.7.7.0-0-dist.deb  #ingored the error output
#export PYTHON=/usr/bin/python2
#/etc/init.d/ambari-server setup
#
#JAVA_HOME=/vagrant/jdk1.8.0_371
#
#Database admin user (postgres): 
#Database name (ambari): 
#Postgres schema (ambari): 
#Username (ambari): 
#Enter Database Password (bigdata):
#
#/etc/init.d/ambari-server start
#tail -f /var/log/ambari-server/ambari-server.log


Latest:
---------------------------------------------------------------------------
Centos AMBARI: 4C/8G: 
swap>=6G:
dd if=/dev/zero of=/myswap.swp bs=1k count=4194304 #æºå¨æ¬èº«æ2Gåswap
mkswap /myswap.swp
swapon /myswap.swp
free -m

VM: 192.168.101.83/84/85
Login on 192.168.101.83:
#https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.8.0
Centos:
yum install -y git python-devel rpm-build gcc-c++

wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg#md5=fe1f997bc722265116870bc7919059ea
sh setuptools-0.6c11-py2.7.egg

#https://stackoverflow.com/questions/61654584/ambari-admin-view-2-7-5-0-0-build-failure
#ambari-admin/pom.xmlï¼(åä¿®æ¹ï¼åç¼è¯)
#  <configuration>
#          <nodeVersion>v11.10.0</nodeVersion>
#          <npmVersion>6.7.0</npmVersion>
#          <workingDirectory>src/main/resources/ui/admin-web/</workingDirectory>
#          <npmInheritsProxyConfigFromMaven>false</npmInheritsProxyConfigFromMaven>
#   </configuration>

#wget https://mirrors.aliyun.com/nodejs-release/v12.22.1/node-v12.22.1-linux-x64.tar.gz
#tar -zxvf node-v12.22.1-linux-x64.tar.gz -C /opt/
#ln -s /opt/node-v12.22.1-linux-x64/ /opt/nodejs

wget https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.gz
tar zxvf node-v16.13.2-linux-x64.tar.gz -C /opt/
ln -s /opt/node-v16.13.2-linux-x64/ /opt/nodejs

cat /etc/profile.d/java.sh 
#!/bin/bash

export JAVA_HOME=/Developer/jdk1.8.0_371
export M2_HOME=/Developer/apache-maven-3.6.3
export _JAVA_OPTIONS="-Xms4g -Xmx4g -Djava.awt.headless=true"
export PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH

---------------
(Custom JDK must be installed on echo machine)
scp -r /Developer/jdk1.8.0_371 root@192.168.101.84:/Developer/
scp -r /Developer/jdk1.8.0_371 root@192.168.101.85:/Developer/
scp -r java.sh root@192.168.101.84:/etc/profile.d/
scp -r java.sh root@192.168.101.85:/etc/profile.d/
----------------

Login on 192.168.101.83:
#https://cloud.tencent.com/developer/article/1375511
/usr/sbin/ambari-server: line 34: buildNumber: unbound variable
vim /usr/sbin/ambari-serverå°${buildNumber}è¿è¡æ¢æ HASH="${VERSION}"

ambari-server setup
JAVA_HOME=/vagrant/jdk1.8.0_371

Database admin user (postgres): 
Database name (ambari): 
Postgres schema (ambari): 
Username (ambari): 
Enter Database Password (bigdata):

http://192.168.101.83:8080/
adminãadmin

bigtopé®é¢è§£å³ï¼
wget https://www.zhangjc.com/images/20210817/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
mvn install:install-file -Dfile=./pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar -DgroupId=org.pentaho -DartifactId=pentaho-aggdesigner-algorithm -Dversion=5.1.5-jhyde -Dpackaging=jar

wget https://packages.confluent.io/maven/io/confluent/kafka-schema-registry-client/6.2.2/kafka-schema-registry-client-6.2.2.jar
mvn install:install-file -Dfile=./kafka-schema-registry-client-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-schema-registry-client -Dversion=6.2.2 -Dpackaging=jar

mvn install:install-file -Dfile=./kafka-clients-2.8.1.jar -DgroupId=org.apache.kafka -DartifactId=kafka-clients -Dversion=2.8.1 -Dpackaging=jar

cd dl/
tar zxf flink-1.15.3.tar.gz
rm -fr flink-1.15.3/flink-formats/flink-avro-confluent-registry/src/test/
rm -fr flink-1.15.3/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test
rm -fr flink-1.15.3.tar.gz
tar -zcf flink-1.15.3.tar.gz flink-1.15.3
rm -fr flink-1.15.3
rm -fr /Developer/bigtop-3.2.0/build/flink/


tar zxf hadoop-3.3.4.tar.gz
vim hadoop-3.3.4-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xm
<nodejs.version>v14.0.0</nodejs.version>
rm -fr hadoop-3.3.4.tar.gz && tar -zcf hadoop-3.3.4.tar.gz hadoop-3.3.4-src && rm -fr hadoop-3.3.4-src
rm -fr /Developer/bigtop-3.2.0/build/hadoop/

#ç¼è¯
./gradlew bigtop-groovy-rpm bigtop-jsvc-rpm bigtop-select-rpm bigtop-utils-rpm flink-rpm hadoop-rpm hbase-rpm hive-rpm kafka-rpm solr-rpm spark-rpm tez-rpm zeppelin-rpm zookeeper-rpm -Dbuildwithdeps=true -PparentDir=/usr/bigtop -PpkgSuffix
#./gradlew allcleanä¼æ¸çç¹build/ä¸å·²ç»æåå¥½çrpmæä»¶ï¼æç¨ã


wget https://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/6.2.2/kafka-avro-serializer-6.2.2.jar
mvn install:install-file -Dfile=./kafka-avro-serializer-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-avro-serializer -Dversion=6.2.2 -Dpackaging=jar

sshæéï¼
#Working all
groupadd hadoop
useradd -m -g hadoop hadoop
passwd hadoop
chmod +w /etc/sudoers
#vim /etc/sudoers
#å¨ sudoers æä»¶ä¸­æ·»å ä»¥ä¸åå®¹
echo "hadoop ALL=(root)NOPASSWD: ALL" >> /etc/sudoers
#æåä¿å­åå®¹åéåº,å¹¶åæ¶ sudoers æä»¶çåæé
chmod -w /etc/sudoers

#Not necessaryï¼åå¯ç ç»å½
#Working on 192.168.101.83
sudo su - hadoop
ssh-keygen -t rsa
#ç´æ¥åå¥å°~/.ssh/authorized_keysä¸­ï¼
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@192.168.101.83
#cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
sudo chmod 700 ~/.ssh
sudo chmod 600 ~/.ssh/authorized_keys
å¹¶å¤å¶å°æææºå¨
scp ~/.ssh/authorized_keys hadoop@192.168.101.84:~/.ssh/
scp ~/.ssh/authorized_keys hadoop@192.168.101.85:~/.ssh/
æ³è¦ææçæºå¨é½äºç¸æéçè¯ï¼
scp ~/.ssh/id_rsa* hadoop@192.168.101.84:~/.ssh/
scp ~/.ssh/id_rsa* hadoop@192.168.101.85:~/.ssh/

ntpï¼
https://www.cnblogs.com/Sungeek/p/10197345.html
sudo yum -y install ntp
sudo timedatectl set-timezone Asia/Shanghai
192.168.101.83ï¼
vim /etc/ntp.conf

restrict 0.0.0.0 mask 0.0.0.0 nomodify notrap
server 127.127.1.0
fudge  127.127.1.0 stratum 10

æéç½®æä»¶ä¸é¢åè¡æ³¨éæï¼
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

ç¶åå¨ä¸é¢æ·»å è¿å è¡ï¼
server 0.cn.pool.ntp.org iburst
server 1.cn.pool.ntp.org iburst
server 2.cn.pool.ntp.org iburst
server 3.cn.pool.ntp.org iburst

systemctl start ntpd
systemctl enable ntpd

æ¥è¯¢ntpæ¯å¦åæ­¥
ntpq -p

NTPå®¢æ·ç«¯éç½®ï¼192.168.101.84/85

[root@localhost ~]# vim /etc/ntp.conf
#éç½®åè®¸NTP Serveræ¶é´æå¡å¨ä¸»å¨ä¿®æ¹æ¬æºçæ¶é´
restrict 192.168.101.83 nomodify notrap noquery
#æ³¨éæå¶ä»æ¶é´æå¡å¨
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#éç½®æ¶é´æå¡å¨ä¸ºæ¬å°æ­å»ºçNTP Serveræå¡å¨
server 192.168.101.83

systemctl start ntpd
systemctl enable ntpd

åæ­¥ï¼
ntpdate -u 192.168.101.83
sudo ntpstat

#æææºå¨
echo "192.168.101.83 node1
192.168.101.84 node2
192.168.101.85 node3" >> /etc/hosts

