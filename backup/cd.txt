CD:

1. ArgoCD <--- ok
2. FluxCD <--- Not Recommend
3. JenkinsX <--- Not Recommend
4. aliyun
5. GitlabCI <--- ok
6. tekton <--- based on k8s, more completed than others.


Prometheus
kustomize <--- ok

rancher <--- ok
k3s/AutoK3s <--- ok
---------------------------------------------------------------------------------
K3s:(Recommend)
#https://docs.k3s.io/quick-start
#https://github.com/k3s-io/k3s/issues/1160
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --disable traefik" sh
cp -a /etc/rancher/k3s/k3s.yaml ~/.kube/config
sed -i 's;127.0.0.1;192.168.101.82;g' ~/.kube/config
kubectl get all -A -o wide

#https://blog.thenets.org/how-to-create-a-k3s-cluster-with-nginx-ingress-controller/
#https://blog.csdn.net/weixin_45444133/article/details/116952250
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml

#Unstalling k3s
/usr/local/bin/k3s-uninstall.sh
---------------------------------------------------------------------------------
#AutoK3s:(Not Recommend)
##https://docs.rancher.cn/docs/k3s/autok3s/_index/
##https://jasonkayzk.github.io/2022/10/22/%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2autok3s/

#For docker:(Recommend)
##docker run -itd --restart=unless-stopped --net host -v /var/run/docker.sock:/var/run/docker.sock cnrancher/autok3s:v0.6.0

##Installing on hosted machine:(optional)
#curl -sS https://rancher-mirror.rancher.cn/autok3s/install.sh  | INSTALL_AUTOK3S_MIRROR=cn sh
##Starting
#autok3s serve --bind-address 192.168.101.82 --bind-port 8080
##Uninstalling:
#/usr/local/bin/autok3s-uninstall.sh

##Install instance:
#Put "--disable traefik" param into "Master Extra Args"
##execute once:
#k get ns --insecure-skip-tls-verify
#k get ns`
---------------------------------------------------------------------------------
# Create a test Namespace, if not exist
kubectl create namespace test

# Apply the example file
#https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/
kubectl -n test apply -f my-example.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-nginx-app
  namespace: test
spec:
  selector:
    matchLabels:
      name: test-nginx-backend
  template:
    metadata:
      labels:
        name: test-nginx-backend
    spec:
      containers:
        - name: backend
          image: docker.io/nginx:alpine
          imagePullPolicy: Always
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: test-nginx-service
  namespace: test
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    name: test-nginx-backend

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-nginx-ingress
  namespace: test
spec:
  rules:
  - host: test.w1.thenets.org
    http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: test-nginx-service
              port: 
                number: 80
  ingressClassName: nginx


#argocd
#https://argo-cd.readthedocs.io/en/stable/getting_started/
#https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&mid=2247512193&idx=1&sn=da41bb4072870e34bdf338c22bcbc8cc&chksm=fbedf04ccc9a795a08f4b0deb5a8518aa901dc1e8678277d232fff0d05ba1613a3f8d8636ab9&scene=178&cur_album_id=2470838961377427457#rd

kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

#Service Type Load Balancer¶
#Change the argocd-server service type to LoadBalancer:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
kubectl -n argocd get svc

#Change port to 8443 and 8080
kubectl -n argocd edit svc argocd-serve
  ports:
  - name: http
    nodePort: 31291
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: https
    nodePort: 31592
    port: 8443
    protocol: TCP
    targetPort: 8080


https://192.168.101.82:8443/

get password:
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo

login name is admin

git clone http://gitlab.zerofinance.net/dave.zhao/fleet_demo.git

#argo-rollouts
kubectl create namespace argo-rollouts
kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml

#demo
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/rollout.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/basic/service.yaml

#Watch
kubectl argo rollouts get rollout rollouts-demo -w

#Updating a Rollout
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:yellow

#Promoting a Rollout
kubectl argo rollouts promote rollouts-demo

#Updating a red Rollout
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:red

#Aborting a Rollout
kubectl argo rollouts abort rollouts-demo

#In order to make Rollout considered Healthy again and not Degraded, it is necessary to change the desired state back to the previous, stable versio
kubectl argo rollouts set image rollouts-demo \
  rollouts-demo=argoproj/rollouts-demo:yellow
  

#ingress:
#https://argoproj.github.io/argo-rollouts/getting-started/nginx/
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/rollout.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/services.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-rollouts/master/docs/getting-started/nginx/ingress.yaml

#cat ingress.yaml has to be changed from >=1.19 cluster
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rollouts-demo-stable
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: rollouts-demo.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field
          service:
            name: rollouts-demo-stable
            port:


#Perform an update
kubectl argo rollouts set image rollouts-demo rollouts-demo=argoproj/rollouts-demo:yellow


#dashboard
kubectl argo rollouts dashboard
http://192.168.101.82:3100/rollouts

#bluegreen
#https://github.com/argoproj/argo-rollouts/blob/master/examples/rollout-bluegreen.yaml

#cat rollout-bluegreen.yaml
# This example demonstrates a Rollout using the blue-green update strategy, which contains a manual
# gate before promoting the new stack.
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: rollout-bluegreen
spec:
  replicas: 2
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: rollout-bluegreen
  template:
    metadata:
      labels:
        app: rollout-bluegreen
    spec:
      containers:
      - name: rollouts-demo
        image: argoproj/rollouts-demo:blue
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
  strategy:
    blueGreen: 
      # activeService specifies the service to update with the new template hash at time of promotion.
      # This field is mandatory for the blueGreen update strategy.
      activeService: rollout-bluegreen-active
      # previewService specifies the service to update with the new template hash before promotion.
      # This allows the preview stack to be reachable without serving production traffic.
      # This field is optional.
      previewService: rollout-bluegreen-preview
      # autoPromotionEnabled disables automated promotion of the new stack by pausing the rollout
      # immediately before the promotion. If omitted, the default behavior is to promote the new
      # stack as soon as the ReplicaSet are completely ready/available.
      # Rollouts can be resumed using: `kubectl argo rollouts promote ROLLOUT`
      autoPromotionEnabled: false

---
kind: Service
apiVersion: v1
metadata:
  name: rollout-bluegreen-active
spec:
  selector:
    app: rollout-bluegreen
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

---
kind: Service
apiVersion: v1
metadata:
  name: rollout-bluegreen-preview
spec:
  selector:
    app: rollout-bluegreen
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rollouts-bluegreen-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: rollouts-bluegreen.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          # Reference to a Service name, also specified in the Rollout spec.strategy.canary.stableService field
          service:
            name: rollout-bluegreen-active
            port:
              number: 80


kubectl argo rollouts set image rollout-bluegreen \
  rollouts-demo=argoproj/rollouts-demo:yellow

  kubectl argo rollouts get rollout rollout-bluegreen -w



#https://docs.gitlab.com/runner/install/docker.html
#https://docs.gitlab.com/runner/register/index.html#docker

#cat /etc/gitlab-runner/config.toml
concurrent = 1
check_interval = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "my-runner"
  url = "http://gitlab.zerofinance.net/"
  token = "111111"
  executor = "docker"
  [runners.custom_build_dir]
  [runners.cache]
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
  [runners.docker]
    tls_verify = false
    image = "docker:latest"
    privileged = true
    disable_entrypoint_overwrite = false
    oom_kill_disable = false
    disable_cache = false
    volumes = ["/var/run/docker.sock:/var/run/docker.sock","/cache","/works/config/runner:/runner"]
    shm_size = 0

docker run -d --name gitlab-runner --restart always \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /works/config/runner:/etc/gitlab-runner \
  gitlab/gitlab-runner:latest

#Register runner
docker run --rm -it -v /works/config/runner:/etc/gitlab-runner gitlab/gitlab-runner register

#https://blog.csdn.net/lenkty/article/details/124668164
#https://blog.csdn.net/boling_cavalry/article/details/106991691
#https://blog.csdn.net/sandaawa/article/details/112897733
#https://github.com/lonly197/docs/blob/master/src/operation/GitLab%20CI%20%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90.md
curl -X POST \
     -F token=a27c33cb07b863f0ecfc80f3650b50 \
     -F ref=1.0.x \
     -F variables[project]=alertmanager-webhook \
     http://gitlab.zerofinance.net/api/v4/projects/575/trigger/pipeline
     

Installing on Linux:
#https://docs.gitlab.com/runner/install/linux-manually.html
sudo curl -L --output /usr/local/bin/gitlab-runner "https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64"
sudo chmod +x /usr/local/bin/gitlab-runner
sudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash
sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner
sudo gitlab-runner start
#Remember git password,login as gitlab-runner:
git config --global credential.helper store
#Mocking cloning a certain repo, input username and password to store credential.

#https://docs.gitlab.com/runner/shells/index.html#shell-profile-loading
To troubleshoot this error, check /home/gitlab-runner/.bash_logout. For example, if the .bash_logout file has a script section like the following, comment it out and restart the pipeline:

if [ "$SHLVL" = 1 ]; then
    [ -x /usr/bin/clear_console ] && /usr/bin/clear_console -q
fi

#gitlab-runner register --name native-runner --url http://gitlab.zerofinance.net/ --registration-token 1111111111

choice shell as a executor.

sudo gitlab-runner register

--------------------------------------------------------------------------------------------------
#AMBARI Ubuntu(didn't work):
##https://cwiki.apache.org/confluence/display/AMBARI/Ambari+Development
#apt-get install build-essential
#apt-get install rpm
#
#python2:
#wget https://www.python.org/ftp/python/2.7.17/Python-2.7.17.tg
#tar zxvf Python-2.7.17.tgz
#cd Python-2.7.17/
#./configure
#make
#make install
#ln -s /usr/local/bin/python /usr/bin/python
##python --version
##Python 2.7.17

##https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.7.7
#wget https://www-eu.apache.org/dist/ambari/ambari-2.7.7/apache-ambari-2.7.7-src.tar.gz
#tar xfvz apache-ambari-2.7.7-src.tar.gz
#cd apache-ambari-2.7.7-src
#mvn versions:set -DnewVersion=2.7.7.0.0
# 
#pushd ambari-metrics
#mvn versions:set -DnewVersion=2.7.7.0.0
#popd
-------------------------------------------

修改settings.xml： #https://gist.github.com/reedv/2d6805e67b93fa7c743c4ff6c8e345dc
    <mirrors>
        <mirror>
            <id>nexus</id>
            <name>internalNexusRepository</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url> 
            <mirrorOf>central</mirrorOf>
        </mirror>
        <mirror>
                <id>nexus-hortonworks</id>
                <mirrorOf>*,!central</mirrorOf>
                <name>Nexus hortonworks</name>
                <url>https://repo.hortonworks.com/content/groups/public/</url>
        </mirror>
    </mirrors>

#修改：
##https://stackoverflow.com/questions/61654584/ambari-admin-view-2-7-5-0-0-build-failure
#ambari-admin/pom.xml：
#<nodeVersion>v4.5.0</nodeVersion>改为：
#<nodeVersion>v11.10.0</nodeVersion>
#
##https://www.cnblogs.com/shine-rainbow/p/16149430.html
#
#无法访问org.json.simple.JSONAware
#<dependency>
#      <groupId>com.googlecode.json-simple</groupId>
#      <artifactId>json-simple</artifactId>
#      <version>1.1</version>
#</dependency>
#
##由于编译环境有关jar包不容易找到(storm-core-0.10.0.2.3.0.0-2557.jar、zookeeper-3.4.5.1.3.0.0-107.jar、zookeeper-3.4.6.2.3.0.0-2557.jar)。
##贴出ambari编译的maven repository,将该repository替换成.m2/repostiroy目录即可。
##scp -r repository.tar.gz root@192.168.101.82:/Developer/.m2/
#cd /Developer/.m2/
#tar zxvf repository.tar.gz
#
#wget https://repo.hortonworks.com/content/repositories/releases/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/storm-core-0.10.0.2.3.0.0-2557.jar -O /Developer/.m2/repository/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/storm-core-0.10.0.2.3.0.0-2557.jar
#确保/Developer/.m2/repository/org/apache/storm/storm-core/0.10.0.2.3.0.0-2557/下面只有一个jar文件
#确保/Developer/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5.1.3.0.0-107/下面只有一个jar文件
#
#cd /data/ambari/apache-ambari-2.7.7-src/
##mvn clean
#mvn -B clean install jdeb:jdeb -DnewVersion=2.7.7.0.0 -DbuildNumber=388e072381e71c7755673b7743531c03a4d61be8 -DskipTests -Drat.skip -Dpython.ver="python >= 2.6"


--------------------------------------------
#cd /data/vagrant/boxes/docker/ubuntu22.04/
#vagrant up
#
##login 192.168.101.83
#cd /vagrant/
#apt install python2 postgresql
#dpkg -i ambari-server_2.7.7.0-0-dist.deb  #ingored the error output
#export PYTHON=/usr/bin/python2
#/etc/init.d/ambari-server setup
#
#JAVA_HOME=/vagrant/jdk1.8.0_371
#
#Database admin user (postgres): 
#Database name (ambari): 
#Postgres schema (ambari): 
#Username (ambari): 
#Enter Database Password (bigdata):
#
#/etc/init.d/ambari-server start
#tail -f /var/log/ambari-server/ambari-server.log


Latest:
---------------------------------------------------------------------------
Centos AMBARI: 4C/8G: 
swap>=6G:
dd if=/dev/zero of=/myswap.swp bs=1k count=4194304 #机器本身有2G发swap
mkswap /myswap.swp
swapon /myswap.swp
free -m

VM: 192.168.101.83/84/85
Login on 192.168.101.83:
#https://cwiki.apache.org/confluence/display/AMBARI/Installation+Guide+for+Ambari+2.8.0
Centos:
yum install -y git python-devel rpm-build gcc-c++

wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg#md5=fe1f997bc722265116870bc7919059ea
sh setuptools-0.6c11-py2.7.egg

#https://stackoverflow.com/questions/61654584/ambari-admin-view-2-7-5-0-0-build-failure
#ambari-admin/pom.xml：(先修改，再编译)
#  <configuration>
#          <nodeVersion>v11.10.0</nodeVersion>
#          <npmVersion>6.7.0</npmVersion>
#          <workingDirectory>src/main/resources/ui/admin-web/</workingDirectory>
#          <npmInheritsProxyConfigFromMaven>false</npmInheritsProxyConfigFromMaven>
#   </configuration>

#wget https://mirrors.aliyun.com/nodejs-release/v12.22.1/node-v12.22.1-linux-x64.tar.gz
#tar -zxvf node-v12.22.1-linux-x64.tar.gz -C /opt/
#ln -s /opt/node-v12.22.1-linux-x64/ /opt/nodejs

wget https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.gz
tar zxvf node-v16.13.2-linux-x64.tar.gz -C /opt/
ln -s /opt/node-v16.13.2-linux-x64/ /opt/nodejs

cat /etc/profile.d/java.sh 
#!/bin/bash

export JAVA_HOME=/Developer/jdk1.8.0_371
export M2_HOME=/Developer/apache-maven-3.6.3
export _JAVA_OPTIONS="-Xms4g -Xmx4g -Djava.awt.headless=true"
export PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH

---------------
(Custom JDK must be installed on echo machine)
scp -r /Developer/jdk1.8.0_371 root@192.168.101.84:/Developer/
scp -r /Developer/jdk1.8.0_371 root@192.168.101.85:/Developer/
scp -r java.sh root@192.168.101.84:/etc/profile.d/
scp -r java.sh root@192.168.101.85:/etc/profile.d/
----------------

Login on 192.168.101.83:
#https://cloud.tencent.com/developer/article/1375511
/usr/sbin/ambari-server: line 34: buildNumber: unbound variable
vim /usr/sbin/ambari-server将${buildNumber}这行换成 HASH="${VERSION}"

ambari-server setup
JAVA_HOME=/vagrant/jdk1.8.0_371

Database admin user (postgres): 
Database name (ambari): 
Postgres schema (ambari): 
Username (ambari): 
Enter Database Password (bigdata):

http://192.168.101.83:8080/
admin、admin

bigtop问题解决：
wget https://www.zhangjc.com/images/20210817/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar
mvn install:install-file -Dfile=./pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar -DgroupId=org.pentaho -DartifactId=pentaho-aggdesigner-algorithm -Dversion=5.1.5-jhyde -Dpackaging=jar

wget https://packages.confluent.io/maven/io/confluent/kafka-schema-registry-client/6.2.2/kafka-schema-registry-client-6.2.2.jar
mvn install:install-file -Dfile=./kafka-schema-registry-client-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-schema-registry-client -Dversion=6.2.2 -Dpackaging=jar

mvn install:install-file -Dfile=./kafka-clients-2.8.1.jar -DgroupId=org.apache.kafka -DartifactId=kafka-clients -Dversion=2.8.1 -Dpackaging=jar

cd dl/
tar zxf flink-1.15.3.tar.gz
rm -fr flink-1.15.3/flink-formats/flink-avro-confluent-registry/src/test/
rm -fr flink-1.15.3/flink-end-to-end-tests/flink-end-to-end-tests-common-kafka/src/test
rm -fr flink-1.15.3.tar.gz
tar -zcf flink-1.15.3.tar.gz flink-1.15.3
rm -fr flink-1.15.3
rm -fr /Developer/bigtop-3.2.0/build/flink/


tar zxf hadoop-3.3.4.tar.gz
vim hadoop-3.3.4-src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xm
<nodejs.version>v14.0.0</nodejs.version>
rm -fr hadoop-3.3.4.tar.gz && tar -zcf hadoop-3.3.4.tar.gz hadoop-3.3.4-src && rm -fr hadoop-3.3.4-src
rm -fr /Developer/bigtop-3.2.0/build/hadoop/

#编译
./gradlew bigtop-groovy-rpm bigtop-jsvc-rpm bigtop-select-rpm bigtop-utils-rpm flink-rpm hadoop-rpm hbase-rpm hive-rpm kafka-rpm solr-rpm spark-rpm tez-rpm zeppelin-rpm zookeeper-rpm -Dbuildwithdeps=true -PparentDir=/usr/bigtop -PpkgSuffix
#./gradlew allclean会清理点build/下已经打包好的rpm文件，慎用。


wget https://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/6.2.2/kafka-avro-serializer-6.2.2.jar
mvn install:install-file -Dfile=./kafka-avro-serializer-6.2.2.jar -DgroupId=io.confluent -DartifactId=kafka-avro-serializer -Dversion=6.2.2 -Dpackaging=jar

ssh打通：
#Working all
groupadd hadoop
useradd -m -g hadoop hadoop
passwd hadoop
chmod +w /etc/sudoers
#vim /etc/sudoers
#在 sudoers 文件中添加以下内容
echo "hadoop ALL=(root)NOPASSWD: ALL" >> /etc/sudoers
#最后保存内容后退出,并取消 sudoers 文件的写权限
chmod -w /etc/sudoers

#Not necessary：免密码登录
#Working on 192.168.101.83
sudo su - hadoop
ssh-keygen -t rsa
#直接写入到~/.ssh/authorized_keys中：
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@192.168.101.83
#cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
sudo chmod 700 ~/.ssh
sudo chmod 600 ~/.ssh/authorized_keys
并复制到所有机器
scp ~/.ssh/authorized_keys hadoop@192.168.101.84:~/.ssh/
scp ~/.ssh/authorized_keys hadoop@192.168.101.85:~/.ssh/
想要所有的机器都互相打通的话：
scp ~/.ssh/id_rsa* hadoop@192.168.101.84:~/.ssh/
scp ~/.ssh/id_rsa* hadoop@192.168.101.85:~/.ssh/

ntp：
https://www.cnblogs.com/Sungeek/p/10197345.html
sudo yum -y install ntp
sudo timedatectl set-timezone Asia/Shanghai
192.168.101.83：
vim /etc/ntp.conf

restrict 0.0.0.0 mask 0.0.0.0 nomodify notrap
server 127.127.1.0
fudge  127.127.1.0 stratum 10

把配置文件下面四行注释掉：
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst

然后在下面添加这几行：
server 0.cn.pool.ntp.org iburst
server 1.cn.pool.ntp.org iburst
server 2.cn.pool.ntp.org iburst
server 3.cn.pool.ntp.org iburst

systemctl start ntpd
systemctl enable ntpd

查询ntp是否同步
ntpq -p

NTP客户端配置：192.168.101.84/85

[root@localhost ~]# vim /etc/ntp.conf
#配置允许NTP Server时间服务器主动修改本机的时间
restrict 192.168.101.83 nomodify notrap noquery
#注释掉其他时间服务器
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst
#配置时间服务器为本地搭建的NTP Server服务器
server 192.168.101.83

systemctl start ntpd
systemctl enable ntpd

同步：
ntpdate -u 192.168.101.83
sudo ntpstat

#所有机器
echo "192.168.101.83 node1
192.168.101.84 node2
192.168.101.85 node3" >> /etc/hosts

