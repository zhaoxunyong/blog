---
title: Kubeadm集群搭建
date: 2017-01-05 17:08:32
categories: ["kubernetes"]
tags: ["kubernetes"]
---
传统的集群安装方式还是比如麻烦，比如说添加新的node节点，需要安装kubelet/proxy，还要配置。kubeadm旨在简化这些繁琐的操作。

<!-- more -->

## 环境准备
docker版本为：1.12.6
kubeadm版本为：v1.6.2

|主机IP|主机名称|内存|
|----|--------|-------------|
|192.168.10.6|k8s-master|1024m|
|192.168.10.7|k8s-node1|1024m|
|192.168.10.8|k8s-node2|1024m|

请参考[Kubernetes集群搭建#安装基础软件](Kubernetes集群搭建.html#安装基础软件)

修改主机名
```bash
#192.168.10.6
hostnamectl --static set-hostname k8s-master
sysctl kernel.hostname=k8s-master

echo '192.168.10.6 k8s-master
192.168.10.7 k8s-node1
192.168.10.8 k8s-node2' >> /etc/hosts

#192.168.10.7
hostnamectl --static set-hostname k8s-node1
sysctl kernel.hostname=k8s-node1

echo '192.168.10.6 k8s-master
192.168.10.7 k8s-node1
192.168.10.8 k8s-node2' >> /etc/hosts

#192.168.10.8
hostnamectl --static set-hostname k8s-node2
sysctl kernel.hostname=k8s-node2

echo '192.168.10.6 k8s-master
192.168.10.7 k8s-node1
192.168.10.8 k8s-node2' >> /etc/hosts
```

修改系统参数
```bash
cat >> /etc/sysctl.conf  << EOF
#k8s
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl -p
```

## 安装

### yum安装

每台添加yum源：
```bash
# docker repo
tee /etc/yum.repos.d/docker.repo <<-'EOF'
[docker-repo]
name=Docker Repository
baseurl=http://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/docker-engine/yum/gpg
EOF

# k8s repo
tee /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes-repo]
name=Kubernetes Repository
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF
```

安装：
```bash
yum install -y docker-engine-1.12.6-1.el7.centos.x86_64
#yum install -y kubelet kubectl kubernetes-cni kubeadm
yum install -y kubernetes-cni-0.5.1-0.x86_64 kubelet-1.6.2-0.x86_64 kubectl-1.6.2-0.x86_64 kubeadm-1.6.2-0.x86_64
```

另外也可以自己手动生成rpm包，具体请参考：[Kubeadm rpm安装包制作](Kubeadm rpm安装包制作.html)

## 下载镜像

### 镜像列表
```
docker pull gcr.io/google_containers/kube-proxy-amd64:v1.6.2
docker pull gcr.io/google_containers/kube-apiserver-amd64:v1.6.2
docker pull gcr.io/google_containers/kube-controller-manager-amd64:v1.6.2
docker pull gcr.io/google_containers/kube-scheduler-amd64:v1.6.2
docker pull gcr.io/google_containers/etcd-amd64:3.0.17
docker pull gcr.io/google_containers/kube-discovery-amd64:1.0
docker pull gcr.io/google_containers/kubedns-amd64:1.7
docker pull gcr.io/google_containers/exechealthz-amd64:1.1
docker pull gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
docker pull gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
docker pull gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
docker pull gcr.io/google_containers/pause-amd64:3.0
```

v1.6.2版本:
```bash
images=(kube-proxy-amd64:v1.6.2 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.6.2 kube-controller-manager-amd64:v1.6.2 kube-apiserver-amd64:v1.6.2 etcd-amd64:3.0.17 kube-dnsmasq-amd64:1.14.1 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0)
for imageName in ${images[@]} ; do
  docker pull mritd/$imageName
  docker tag mritd/$imageName gcr.io/google_containers/$imageName
  docker rmi mritd/$imageName
done

```
v1.5.1版本:
```bash
images=(kube-proxy-amd64:v1.5.1 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.1 kube-controller-manager-amd64:v1.5.1 kube-apiserver-amd64:v1.5.1 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 dnsmasq-metrics-amd64:1.0 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0)
for imageName in ${images[@]} ; do
  docker pull mritd/$imageName
  docker tag mritd/$imageName gcr.io/google_containers/$imageName
  docker rmi mritd/$imageName
done
```

#echo "KUBE_REPO_PREFIX=docker.io/mritd" >> /etc/profile
#source /etc/profile

export KUBE_REPO_PREFIX=docker.io/mritd \
         KUBE_DISCOVERY_IMAGE=docker.io/mritd/kube-discovery-amd64:1.0 \
         KUBE_ETCD_IMAGE=docker.io/mritd/etcd-amd64:3.0.14-kubeadm
# kubeadm init --pod-network-cidr="10.24.0.0/16"

## master初始化

### 初始化
```bash
#export KUBE_COMPONENT_LOGLEVEL='--v=0'
kubeadm init --kubernetes-version=v1.6.2 --apiserver-advertise-address=192.168.10.6
```

如果是使用flannel网络的话，要加上--pod-network-cidr 10.244.0.0/16
```
kubeadm init --kubernetes-version=v1.6.2 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6
```

### 异常处理
初始化时, 会发现卡死不动, 可以通过系统日志查看错误
```bash
journalctl -f -u kubelet.server
# tail -n100 -f /var/log/messages

failed to create kubelet: misconfiguration: kubelet cgroup driver: "cgroupfs" is different from docker cgroup driver: "systemd"
```

这个是K8S v1.6.x的一个变化, 文件驱动与docker使用的文件驱动不一致, 导致镜像无法启动。
此处可以修改kubelet的文件驱动，请参考[http://www.jianshu.com/p/02dc13d2f651](http://www.jianshu.com/p/02dc13d2f651)
注意: 每台都要修改
```bash
# 进入kubelet启动配置文件
cd /etc/systemd/system/kubelet.service.d/
vi 10-kubeadm.conf
```
将
```bash
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"
```

替换为：
```bash
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
```

然后再重新初始化
```bash
kubeadm reset
kubeadm init --kubernetes-version=v1.6.2 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6
```

正常情况下，应该能显示以下的日志：
```bash
kubeadm init --kubernetes-version=v1.6.2 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.10.6
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.6.2
[init] Using Authorization mode: RBAC
[preflight] Running pre-flight checks
[preflight] WARNING: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Starting the kubelet service
[certificates] Generated CA certificate and key.
[certificates] Generated API server certificate and key.
[certificates] API Server serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.6]
[certificates] Generated API server kubelet client certificate and key.
[certificates] Generated service account token signing key and public key.
[certificates] Generated front-proxy CA certificate and key.
[certificates] Generated front-proxy client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 126.837475 seconds
[apiclient] Waiting for at least one node to register
[apiclient] First node has registered after 6.516528 seconds
[token] Using token: dc0c43.6b8fc1935eb5ddbf
[apiconfig] Created RBAC rules
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  sudo cp /etc/kubernetes/admin.conf $HOME/
  sudo chown $(id -u):$(id -g) $HOME/admin.conf
  export KUBECONFIG=$HOME/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token dc0c43.6b8fc1935eb5ddbf 192.168.10.6:6443
```

之前的版本, 当我们初始化成功之后, 会发现token不会保留, 如果一旦没有记录下来, 其他节点就没法加入了, 这里添加了kubeadm token命令
```bash
kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                   DESCRIPTION
dc0c43.6b8fc1935eb5ddbf   <forever>   <never>   authentication,signing   The default bootstrap token generated by 'kubeadm init'.
```

默认情况下, master节点是不会调度pod, 也就是说, 只有一台主机的情况下, 我们无法启动pod, 但有的时候我们的确只有一台机器, 这个时候可以执行命令, 允许master调度pod(这个命令和1.5.x版本不一样)
```bash
kubectl taint nodes --all node-role.kubernetes.io/master-
```

### 查询node情况
```bash
kubectl --kubeconfig=/etc/kubernetes/kubelet.conf get nodes
```

### kubectl 命令
这个命令是我们经常使用的, 几乎所有的k8s相关操作都需要, 但当我们集群安装好后, 发现这个命令会报错。
最直接的方法是带上参数 --kubeconfig
```
kubectl --kubeconfig=/etc/kubernetes/kubelet.conf get nodes
```

如果不想每次都带上参数, 可以配置环境变量
```bash
vi ~/.bash_profile
# 添加
export KUBECONFIG=/etc/kubernetes/kubelet.conf
```

执行
```bash
source ~/.bash_profile
```

这样就可以不用带--kubeconfig参数了
```bash
kubectl get nodes
```

## node加入

请记得init后的join命令(类似于下面，但token不一样)，其他的node要加入集群的话，必须用下面的命令：
```
kubeadm join --token dc0c43.6b8fc1935eb5ddbf 192.168.10.6:6443

[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Running pre-flight checks
[preflight] WARNING: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Starting the kubelet service
[discovery] Trying to connect to API Server "192.168.10.6:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.10.6:6443"
[discovery] Cluster info signature and contents are valid, will use API Server "https://192.168.10.6:6443"
[discovery] Successfully established connection with API Server "192.168.10.6:6443"
[bootstrap] Detected server version: v1.6.2
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server, generating KubeConfig...
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.
```

查看node信息
```bash
 kubectl get no
NAME         STATUS     AGE       VERSION
k8s-master   NotReady   39m       v1.6.2
k8s-node1    NotReady   32s       v1.6.2
k8s-node2    NotReady   53s       v1.6.2
```

## 创建网络
没创建网络时，kube-dns是不能正常运行的，STATUS为Pending。
```bash
[root@k8s-master ~]# kubectl get pod -o wide -n kube-system
NAME                                 READY     STATUS    RESTARTS   AGE       IP             NODE
etcd-k8s-master                      1/1       Running   0          37m       192.168.10.6   k8s-master
kube-apiserver-k8s-master            1/1       Running   0          37m       192.168.10.6   k8s-master
kube-controller-manager-k8s-master   1/1       Running   0          37m       192.168.10.6   k8s-master
kube-dns-3913472980-gx5zn            0/3       Pending   0          42m       <none>         
kube-proxy-3970g                     1/1       Running   0          4m        192.168.10.8   k8s-node2
kube-proxy-t8zhh                     1/1       Running   0          42m       192.168.10.6   k8s-master
kube-proxy-xvsdk                     1/1       Running   0          3m        192.168.10.7   k8s-node1
kube-scheduler-k8s-master            1/1       Running   0          37m       192.168.10.6   k8s-master
```

























kubectl create -f kube-flannel-rbac.yml
Error from server (Forbidden): error when creating "kube-flannel-rbac.yml": User "system:node:k8s-master" cannot create clusterroles.rbac.authorization.k8s.io at the cluster scope. (post clusterroles.rbac.authorization.k8s.io)
Error from server (Forbidden): error when creating "kube-flannel-rbac.yml": User "system:node:k8s-master" cannot create clusterrolebindings.rbac.authorization.k8s.io at the cluster scope. (post clusterrolebindings.rbac.authorization.k8s.io)
[root@k8s-master kubelet.service.d]# proxy_on
已开启代理
[root@k8s-master kubelet.service.d]# kubectl create -f kube-flannel-rbac.yml


etcdctl mkdir /coreos.com/network
etcdctl set /coreos.com/network/config '{"Network":"172.17.0.0/16"}'

etcdctl set /coreos.com/network/config '{"NetWork":"10.244.0.0/16"}'
etcdctl get /coreos.com/network/config

--etcd-servers=http://127.0.0.1:2379





### 重置
如果出现问题，可以通过以下方式解决：
```bash
#reset请小心，所删除所有的kube配置，也适用于node：
kubeadm reset
```

删除网络
```bash
ifconfig  cni0 down
brctl delbr cni0
ip link delete flannel.1

ip link delete cni0 
ip link delete flannel.1
ip link delete weave
```

清空防火墙
```bash
iptables -L -n
iptables -t nat -S 

iptables -F
iptables -X
iptables -Z
iptables -t nat -F
iptables -t nat -X
iptables -t nat -Z
```

再重新创建即可。





网络模式有几种：
参考：
> http://blog.dataman-inc.com/shurenyun-docker-133/
> http://cmgs.me/life/docker-network-cloud
> http://dockone.io/article/1115
> http://blog.liuker.cn/index.php/docker/30.html
1. 直接路由+quagga
2. calico
3. flannel
4. weave

不建议weave，性能不太好。

此处以flannel为例。

```bash
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml -O kube-flannel.yaml
kubectl create -f kube-flannel.yaml
```







kubeadm join --token=c91e56.f6f482e1e3900b38 192.168.10.6

kubectl apply -f https://git.io/weave-kube

kubectl远程访问：
export KUBERNETES_MASTER=https://192.168.10.6:6443
kubectl --kubeconfig=/etc/kubernetes/admin.conf get node


kubectl scale deployment elasticsearch --replicas=0 -n kube-system
kubectl get deployment -o wide
或者
kubectl get deploy -o wide
kubectl exec -it busybox -- nslookup kubernetes.default
kubectl exec -it busybox -- ping redis-master
kubectl exec -it busybox -- cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

kube-dns-2924299975-kdgb3               2/4       CrashLoopBackOff   30         53m       10.40.0.1     k8s-master


## 参考
> https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm
> https://mritd.me/2016/11/21/kubeadm-other-problems