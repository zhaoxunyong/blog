---
title: Kubeadm集群搭建
date: 2017-01-05 17:08:32
categories: ["kubernetes"]
tags: ["kubernetes"]
---
传统的集群安装方式还是比如麻烦，比如说添加新的node节点，需要安装kubelet/proxy，还要配置。kubeadm旨在简化这些繁琐的操作。目前kubeadm还处于1.6.0-0.alpha版本，不推荐生产环境使用。

## 环境准备
docker版本为：1.12.5
kubeadm版本为：v1.5.1

|主机IP|主机名称|内存|
|----|--------|-------------|
|192.168.10.6|k8s-master|1024m|
|192.168.10.7|k8s-node1|1024m|

请参考[Kubernetes集群搭建#安装基础软件](Kubernetes集群搭建.html#安装基础软件)

修改主机名
```bash
#192.168.10.6
hostnamectl --static set-hostname k8s-master
sysctl kernel.hostname=k8s-master

echo '192.168.10.6 k8s-master
192.168.10.7 k8s-node1
192.168.10.8 k8s-node2' >> /etc/hosts

#192.168.10.7
hostnamectl --static set-hostname k8s-node1
sysctl kernel.hostname=k8s-node1

echo '192.168.10.6 k8s-master
192.168.10.7 k8s-node1
192.168.10.8 k8s-node2' >> /etc/hosts

```

## 安装kubeadm

### yum安装

每台添加yum源：
```bash
tee /etc/yum.repos.d/k8s.repo <<-'EOF'
[k8s-repo]
name=kubernetes Repository
baseurl=https://rpm.mritd.me/centos/7/x86_64
enabled=1
gpgcheck=1
gpgkey=https://cdn.mritd.me/keys/rpm.public.key
EOF
```

安装：
```bash
yum install -y kubelet kubectl kubernetes-cni kubeadm
```

如果这个源不稳定的话，可以下载我创建好的源，直接通过yum localinstall *.rpm方式安装
```bash
git clone https://git.coding.net/zhaoxunyong/repo.git
cd repo/yum/kubeadmin/x86_64
yum -y localinstall kubeadm-1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64.rpm
yum -y localinstall kubectl-1.5.1-0.x86_64.rpm
yum -y localinstall kubelet-1.5.1-0.x86_64.rpm
yum -y localinstall kubernetes-cni-0.3.0.1-0.07a8a2.x86_64.rpm
```

另外也可以自己手动生成rpm包，具体请参考：[Kubeadm rpm安装包制作](Kubeadm rpm安装包制作.html)

## 下载镜像
v1.5.1版本:
```bash
images=(kube-proxy-amd64:v1.5.1 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.1 kube-controller-manager-amd64:v1.5.1 kube-apiserver-amd64:v1.5.1 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 dnsmasq-metrics-amd64:1.0 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0)
for imageName in ${images[@]} ; do
  docker pull mritd/$imageName
  docker tag mritd/$imageName gcr.io/google_containers/$imageName
  docker rmi mritd/$imageName
done

docker pull zhanghepeng/dnsmasq-metrics-amd64:1.0
docker tag zhanghepeng/dnsmasq-metrics-amd64:1.0 gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
docker rmi zhanghepeng/dnsmasq-metrics-amd64:1.0
```

v1.4.6版本：
```bash
images=(kube-proxy-amd64:v1.4.6 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.4.6 kube-controller-manager-amd64:v1.4.6 kube-apiserver-amd64:v1.4.6 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 dnsmasq-metrics-amd64:1.0 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0)
for imageName in ${images[@]} ; do
  docker pull mritd/$imageName
  docker tag mritd/$imageName gcr.io/google_containers/$imageName
  docker rmi mritd/$imageName
done
docker pull zhanghepeng/dnsmasq-metrics-amd64:1.0
docker tag zhanghepeng/dnsmasq-metrics-amd64:1.0 gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
docker rmi zhanghepeng/dnsmasq-metrics-amd64:1.0
```

echo "KUBE_REPO_PREFIX=docker.io/mritd" >> /etc/profile
source /etc/profile

export KUBE_REPO_PREFIX=docker.io/mritd \
         KUBE_DISCOVERY_IMAGE=docker.io/mritd/kube-discovery-amd64:1.0 \
         KUBE_ETCD_IMAGE=docker.io/mritd/etcd-amd64:3.0.14-kubeadm
# kubeadm init --pod-network-cidr="10.24.0.0/16"

## master初始化

### 初始化
```bash
export KUBE_COMPONENT_LOGLEVEL='--v=0'
kubeadm init --api-advertise-addresses 192.168.10.6 --use-kubernetes-version v1.4.6
kubeadm join --token=ad8693.4250d8419cd67485 192.168.10.6
```

如果是使用flannel网络的话，要加上--pod-network-cidr 10.244.0.0/16
```
kubeadm init --api-advertise-addresses 192.168.10.6 --use-kubernetes-version v1.4.6 --pod-network-cidr 10.244.0.0/16
```

*** 重要 ***
请记得init后的join命令(类似于下面，但token不一样)，其他的node要加入集群的话，必须用下面的命令：
```
kubeadm join --token=3b9130.5f8c010730b968ae 192.168.10.6
```

### 重置
如果出现问题，可以通过以下方式解决：
```bash
#reset请小心，所删除所有的kube配置，也适用于node：
kubeadm reset
```

删除网络
```bash
ifconfig  cni0 down
brctl delbr cni0
ip link delete flannel.1

ip link delete cni0 
ip link delete flannel.1
ip link delete weave
```

清空防火墙
```bash
iptables -L -n
iptables -t nat -S 

iptables -F
iptables -X
iptables -Z
iptables -t nat -F
iptables -t nat -X
iptables -t nat -Z
```

再重新创建即可。

## 加入node
```bash
[root@k8s-node1 ~]# kubeadm join --token=c916e6.a91b0cc044cc6d42 192.168.10.6
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[preflight] WARNING: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
[preflight] Starting the kubelet service
[tokens] Validating provided token
[discovery] Created cluster info discovery client, requesting info from "http://192.168.10.6:9898/cluster-info/v1/?token-id=c916e6"
[discovery] Cluster info object received, verifying signature using given token
[discovery] Cluster info signature and contents are valid, will use API endpoints [https://192.168.10.6:6443]
[bootstrap] Trying to connect to endpoint https://192.168.10.6:6443
[bootstrap] Detected server version: v1.5.1
[bootstrap] Successfully established connection with endpoint "https://192.168.10.6:6443"
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server:
Issuer: CN=kubernetes | Subject: CN=system:node:k8s-node1 | CA: false
Not before: 2017-01-07 09:27:00 +0000 UTC Not After: 2018-01-07 09:27:00 +0000 UTC
[csr] Generating kubelet configuration
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.
```

## 创建网络
没创建网络时，kube-dns是不能正常运行的，STATUS为ContainerCreating。
```bash
[root@k8s-master ~]# kubectl get pod -o wide -n kube-system
NAME                                 READY     STATUS    RESTARTS   AGE       IP             NODE
dummy-2088944543-k1qgt               1/1       Running   0          41s       192.168.10.6   k8s-master
etcd-k8s-master                      1/1       Running   1          45s       192.168.10.6   k8s-master
kube-apiserver-k8s-master            1/1       Running   2          46s       192.168.10.6   k8s-master
kube-controller-manager-k8s-master   1/1       Running   1          46s       192.168.10.6   k8s-master
kube-discovery-1769846148-9bc3z      1/1       Running   0          40s       192.168.10.6   k8s-master
kube-dns-2924299975-kqbl0            0/4       Pending   0          38s       <none>         
kube-proxy-9csx1                     1/1       Running   0          38s       192.168.10.6   k8s-master
kube-scheduler-k8s-master            1/1       Running   1          46s       192.168.10.6   k8s-master
```

网络模式有几种：
参考：
> http://blog.dataman-inc.com/shurenyun-docker-133/
> http://cmgs.me/life/docker-network-cloud
> http://dockone.io/article/1115
> http://blog.liuker.cn/index.php/docker/30.html
1. 直接路由+quagga
2. calico
3. flannel
4. weave

不建议weave，性能不太好。

此处以flannel为例。

```bash
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml -O kube-flannel.yaml
kubectl create -f kube-flannel.yaml
```







kubeadm join --token=c91e56.f6f482e1e3900b38 192.168.10.6

kubectl apply -f https://git.io/weave-kube

kubectl远程访问：
export KUBERNETES_MASTER=https://192.168.10.6:6443
kubectl --kubeconfig=/etc/kubernetes/admin.conf get node


kubectl scale deployment elasticsearch --replicas=0 -n kube-system
kubectl get deployment -o wide
或者
kubectl get deploy -o wide
kubectl exec -it busybox -- nslookup kubernetes.default
kubectl exec -it busybox -- ping redis-master
kubectl exec -it busybox -- cat /etc/resolv.conf 
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

kube-dns-2924299975-kdgb3               2/4       CrashLoopBackOff   30         53m       10.40.0.1     k8s-master


## 参考
> https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm
> https://mritd.me/2016/11/21/kubeadm-other-problems