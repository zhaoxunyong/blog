#--------------------------------------------------------------------------
集群安装：

#https://www.kubernetes.org.cn/doc-16
导入images:
cd /docker/works/images/k8s/
./importK8s.sh

docker load -i /docker/works/images/others/redis-master.tar 
docker load -i /docker/works/images/others/guestbook-redis-slave.tar 
docker load -i /docker/works/images/others/guestbook-php-frontend.tar

-------------------

images=(kube-proxy-amd64:v1.4.5 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.5 kube-controller-manager-amd64:v1.4.5 kube-apiserver-amd64:v1.4.5 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)
for imageName in ${images[@]} ; do
  docker pull mritd/$imageName
  docker tag mritd/$imageName gcr.io/google_containers/$imageName
  docker rmi mritd/$imageName
done

master(kubeadmin方式)-------------------------------------------------------
#https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/#section-5
#cd /docker/works/images/k8s/rpm/docker
#yum localinstall *.rpm
yum install docker-engine -y
sed -i "s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd --registry-mirror=http://3fecfd09.m.daocloud.io;" /usr/lib/systemd/system/docker.service

systemctl start docker
systemctl enable docker

#cd /docker/works/images/k8s/rpm/k8s
#yum localinstall *.rpm
yum install -y socat kubelet kubeadm kubectl kubernetes-cni

cd /docker/works/images/k8s
./importK8s.sh

# 写入 hostname(node 节点后缀改成 .node)
#echo "192-168-10-6.master" > /etc/hostname 
hostnamectl --static set-hostname 192-168-10-6.master
hostnamectl命令会自动生成/etc/hostname文件
# 加入 hosts
#echo "127.0.0.1   192-168-10-6.master" >> /etc/hosts
#echo '192.168.10.6 192-168-10-6.master
#192.168.10.7   192-168-10-7.node' >> /etc/hosts
echo '192.168.10.6 k8s-master
192.168.10.7   k8s-node1' >> /etc/hosts
# 不重启情况下使内核生效
#sysctl kernel.hostname=192-168-10-6.master

kubeadm 等相关 rpm 安装后会生成 /etc/kubernetes 目录，而 kubeadm init 时候又会检测这些目录是否存在，如果存在则停止初始化，所以要先清理一下，以下清理脚本来源于 官方文档 Tear down 部分，该脚本同样适用于初始化失败进行重置

systemctl stop kubelet;
# 注意: 下面这条命令会干掉所有正在运行的 docker 容器，
# 如果要进行重置操作，最好先确定当前运行的所有容器都能干掉(干掉不影响业务)，
# 否则的话最好手动删除 kubeadm 创建的相关容器(gcr.io 相关的)
docker rm -f -v $(docker ps -q);
find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;
rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;

systemctl enable kubelet
systemctl start kubelet

kubeadm init --api-advertise-addresses 192.168.10.6 
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[init] Using Kubernetes version: v1.5.1
[tokens] Generated token: "586c91.f045fd055f9b7155"
[certificates] Generated Certificate Authority key and certificate.
[certificates] Generated API Server key and certificate
[certificates] Generated Service Account signing keys
[certificates] Created keys and certificates in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[apiclient] Created API client, waiting for the control plane to become ready
[apiclient] All control plane components are healthy after 15.646161 seconds
[apiclient] Waiting for at least one node to register and become ready
[apiclient] First node is ready after 3.052755 seconds
[apiclient] Creating a test deployment
[apiclient] Test deployment succeeded
[token-discovery] Created the kube-discovery deployment, waiting for it to become ready
[token-discovery] kube-discovery is ready after 9.011433 seconds
[addons] Created essential addon: kube-proxy
[addons] Created essential addon: kube-dns

Your Kubernetes master has initialized successfully!

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node:

kubeadm join --token=586c91.f045fd055f9b7155 192.168.10.6

node-------------------------------------------------------
cd /docker/works/images/k8s/rpm/docker
yum localinstall *.rpm
sed -i "s;^ExecStart=/usr/bin/dockerd$;ExecStart=/usr/bin/dockerd --registry-mirror=http://3fecfd09.m.daocloud.io;" /usr/lib/systemd/system/docker.service

systemctl start docker
systemctl enable docker

cd /docker/works/images/k8s/rpm/k8s
yum localinstall *.rpm

cd /docker/works/images/k8s
./importK8s.sh

#echo "192-168-10-7.node" > /etc/hostname 
#hostnamectl --static set-hostname 192-168-10-7.node
#echo '192.168.10.6 192-168-10-6.master
#192.168.10.7   192-168-10-7.node' >> /etc/hosts
echo '192.168.10.6 k8s-master
192.168.10.7   k8s-node1' >> /etc/hosts
#sysctl kernel.hostname=192-168-10-7.node

# 启动 kubelet
systemctl enable kubelet
systemctl start kubelet

# 初始化加入集群
[root@k8s2 k8s]# kubeadm join --token=586c91.f045fd055f9b7155 192.168.10.6
[kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters.
[preflight] Running pre-flight checks
[tokens] Validating provided token
[discovery] Created cluster info discovery client, requesting info from "http://192.168.10.6:9898/cluster-info/v1/?token-id=a02acc"
[discovery] Cluster info object received, verifying signature using given token
[discovery] Cluster info signature and contents are valid, will use API endpoints [https://192.168.10.6:6443]
[bootstrap] Trying to connect to endpoint https://192.168.10.6:6443
[bootstrap] Detected server version: v1.5.1
[bootstrap] Successfully established connection with endpoint "https://192.168.10.6:6443"
[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request
[csr] Received signed certificate from the API server:
Issuer: CN=kubernetes | Subject: CN=system:node:192-168-10-7.node | CA: false
Not before: 2016-12-26 09:49:00 +0000 UTC Not After: 2017-12-26 09:49:00 +0000 UTC
[csr] Generating kubelet configuration
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run 'kubectl get nodes' on the master to see this machine join.


在master执行：
[root@k8s1 k8s]# kubectl get nodes
NAME                  STATUS         AGE
192-168-10-6.master   Ready,master   13m
192-168-10-7.node     Ready          26s


#--------------------------------------------------------
#http://blog.csdn.net/yanzi1225627/article/details/51121507
#CentOS命令行使用shadowsocks代理的方法:
#yum install python-pip
#pip install shadowsocks
#vi /etc/shadowsocks.json
#{
#"server":"153.125.237.148",
#"server_port":31015,
#"local_address": "127.0.0.1",
#"local_port":1080,
#"password":"Aa123456",
#"timeout":600,
#"method":"aes-256-cfb"
#}
#
##sslocal -c /etc/shadowsocks.json
##To run in the background
#sudo sslocal -c /etc/shadowsocks.json -d start
#
##Auto Start the Client on System Boot
##Edit /etc/rc.local file
##sudo vi /etc/rc.local
##Put the following line above the exit 0 line:
#sudo sslocal -c /etc/shadowsocks.json -d start
#
#安装Privoxy:
#wget http://www.privoxy.org/sf-download-mirror/Sources/3.0.26%20%28stable%29/privoxy-3.0.26-stable-src.tar.gz
#useradd privoxy
#autoheader && autoconf
#./configure
#make && make install
#
#查看vim /usr/local/etc/privoxy/config文件，先搜索关键字:listen-address找到listen-address 127.0.#0.1:8118这一句，保证这一句没有注释，8118就是将来http代理要输入的端口。然后搜索forward-socks5t,将forward-socks5t / 127.0.#0.1:1080 .此句的注释去掉. 
#privoxy --user privoxy /usr/local/etc/privoxy/config
#
#配置/etc/profile
#
#执行vim /etc/profile,添加如下三句:
#
#export http_proxy=http://127.0.0.1:8118
#export https_proxy=http://127.0.0.1:8118
#export ftp_proxy=http://127.0.0.1:8118
#
#source /etc/profile
#
#快速启动：
#sudo sslocal -c /etc/shadowsocks.json -d start
#privoxy --user privoxy /usr/local/etc/privoxy/config

macos/linux终端：(建议用这个)
http://droidyue.com/blog/2016/04/04/set-shadowsocks-proxy-for-terminal/index.html



--------------------------------------------------------------
etcd集群安装：
rpm安装：
yum -y install etcd
版本为：etcd-3.0.15-1.x86_64
systemctl start etcd
systemctl enable etcd
https://mritd.me/2016/09/01/Etcd-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/
http://blog.csdn.net/u010511236/article/details/52386229
etcd1:
grep -v ^# /etc/etcd/etcd.conf 
ETCD_NAME=etcd1
ETCD_DATA_DIR="/var/lib/etcd/etcd1"
ETCD_LISTEN_PEER_URLS="http://192.168.10.6:2380"
ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:2379,http://192.168.10.6:2379,http://192.168.10.6:4001"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.6:2380"
ETCD_DISCOVERY="https://discovery.etcd.io/248e628e9c77cc927dc1cc2749f3c9bf"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.6:2379,http://192.168.10.6:4001"

etcd2:
grep -v ^# /etc/etcd/etcd.conf 
ETCD_NAME=etcd2
ETCD_DATA_DIR="/var/lib/etcd/etcd2"
ETCD_LISTEN_PEER_URLS="http://192.168.10.7:2380"
ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:2379,http://192.168.10.7:2379,http://192.168.10.7:4001"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.7:2380"
ETCD_DISCOVERY="https://discovery.etcd.io/248e628e9c77cc927dc1cc2749f3c9bf"
ETCD_INITIAL_CLUSTER_STATE="exist"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.7:2379,http://192.168.10.7:4001"

etcd3:
grep -v ^# /etc/etcd/etcd.conf 
ETCD_NAME=etcd3
ETCD_DATA_DIR="/var/lib/etcd/etcd3"
ETCD_LISTEN_PEER_URLS="http://192.168.10.8:2380"
ETCD_LISTEN_CLIENT_URLS="http://127.0.0.1:2379,http://192.168.10.8:2379,http://192.168.10.8:4001"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.8:2380"
ETCD_DISCOVERY="https://discovery.etcd.io/248e628e9c77cc927dc1cc2749f3c9bf"
ETCD_INITIAL_CLUSTER_STATE="exist"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.8:2379,http://192.168.10.8:4001"

基于已有集群的服务发现
# 获取集群标识 size 代表要创建的集群大小
curl -w "\n" 'https://discovery.etcd.io/new?size=3'
# 返回如下
https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732

# 设置集群标识,删除掉 ETCD_INITIAL_CLUSTER 字段,添加：
ETCD_DISCOVERY="https://discovery.etcd.io/f6a252c5240cc89b91fa00dac95d5732"
也可以通过已有的集群自动发现：
首先需要在已经搭建的etcd中创建用于发现的url
curl -X PUT http://10.0.1.33:2379/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3

返回：
{"action":"set","node":{"key":"/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size","value":"3","modifiedIndex":170010,"createdIndex":170010}}

如上表示创建一个集群大小为3的etcd发现url，创建成功后按如下配置启动各节点
./etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.111:2380 \
  --listen-peer-urls http://10.0.1.111:2380 \
  --listen-client-urls http://10.0.1.111:2379,http://127.0.0.1:2379 \
  --advertise-client-urls http://10.0.1.111:2379 \ 
 --discovery http://10.0.1.33:2379/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83


出现x509: failed to load system roots and no roots provided时：
http://stackoverflow.com/questions/38551685/kubernetes-dashboard-and-ssl-x509-failed-to-load-system-roots-and-no-roots-pr
container中需要mount目录：
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubernetes-dashboard
  template:
    ...
    spec:
      volumes:
      - name: "etcpki"
        hostPath:
          path: "/etc/pki"
      - name: "config"
        hostPath:
          path: "/k8s"
      containers:
      - name: kubernetes-dashboard
        image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0
        resources:
          # keep request = limit to keep this container in guaranteed class
          limits:
            cpu: 100m
            memory: 50Mi
          requests:
            cpu: 100m
            memory: 50Mi
        env:
        - name: KUBECONFIG
          value: /k8s/.kube/config
        ports:
        - containerPort: 9090
          protocol: TCP
        args:
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          - --apiserver-host=https://kube-apiserver
        volumeMounts:
        - name: "etcpki"
          mountPath: "/etc/pki"
          readOnly: true
        - name: "config"
          mountPath: "/k8s"
          readOnly: true
          ...


configmap:
http://stackoverflow.com/questions/37166822/is-it-a-way-to-add-arbitrary-record-to-kube-dns





